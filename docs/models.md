# Overview

# N-Grams

* 基于已有语料库，统计计算颇为简单，某种程度上，较为符合“语言模型”的初衷，可捕捉到局部语义信息
* 无法捕捉远距离的词之间的关系，如果n设置过大，会导致对语料库的大小需求极大，但这个很难实现。
* 前缀必须严格出现过：但真实世界中自然语言不是这样运作的

# Bag-of-Words

* 损失了顺序信息：这无疑是很关键的一部分信息；
* 向量大小对应于词汇表大小，很多时候会特别大（相对于文档大小而言），从而成为高维稀疏向量，此时很难更好的区分不同文档（余弦相似度使用到内积，而稀疏向量的内积会接近于0）。
* 词（token)表示为one-hot向量，此时不包含任何语义信息，仅仅是一个序号，从而也完全无法区分不同的词
* 也因为上一条，词相互之间同等重要，这在真实问题中很不适合
  - 一个改进是使用类似于 TF-IDF 的方法
* 适用于主题建模、文本分类等任务（文档级、粗粒度，对语序通常较不敏感，较差文档？）
  - 不适合翻译、实体识别等任务 

# Word2Vec

词嵌入：将词映射到向量空间的过程或表示方法。
词向量：词的具体向量表示。通过一些既定的语料库和模型，得到词的向量表示。该表示“嵌入”了词的各种语义（以及其上下文），比如词性、肯定否定、性别等等。

问题：词嵌入的结果，适合比较两个词的“相对”相似度，但无法确定其具体的“内涵”？即该向量究竟包含哪些语义，每个维度究竟意味着什么？

分布式表示：将离散的符号（单词或 token 等）映射到连续的向量空间中。2013年 Mikolov 等人的 Word2vec。



* 一个词与哪些词语义更接近
* 
